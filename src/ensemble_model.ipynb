{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from glob import glob\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "from blazeface import FaceExtractor, BlazeFace, VideoReader\n",
    "from architectures import fornet\n",
    "from architectures.fornet import FeatureExtractor\n",
    "from utils import utils\n",
    "from utils.utils import get_transformer\n",
    "from utils.utils import plot_confusion_matrix\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import pprint\n",
    "import cv2\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select architecture, device, face policy, face size, frames per video, dataset and provide model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_choices = ['TimmV2', 'TimmV2ST', 'ViT', 'ViTST']\n",
    "choices = {'v2': 'TimmV2', 'v2st': 'TimmV2ST', 'vit': 'ViT', 'vitst': 'ViTST'}\n",
    "device = torch.device(\n",
    "    'cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "face_policy = 'scale'\n",
    "face_size = 224\n",
    "frames_per_video = 32  # -1 means entire video\n",
    "\n",
    "dataset = \"ffpp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provide path to video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frames(path):\n",
    "    video = cv2.VideoCapture(path)\n",
    "    return int(video.get(cv2.CAP_PROP_FRAME_COUNT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['043.mp4', '091.mp4', '150.mp4', '250.mp4', '377.mp4', '488.mp4', '522.mp4', '666.mp4', '777.mp4', '881.mp4', '992.mp4']\n",
      "[0, 1]\n",
      "[495, 639, 456, 564, 398, 518, 713, 383, 325, 638, 718]\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "video_paths = glob('../sample_videos/ffpp/real/**/*.mp4', recursive=True)\n",
    "file_names = []\n",
    "fpv = []\n",
    "for i in video_paths:\n",
    "    file_names.append(os.path.basename(i))\n",
    "    fpv.append( count_frames(i)  )\n",
    "file_names.sort()\n",
    "len(file_names)\n",
    "print(file_names)\n",
    "\n",
    "video_idxs = [0,1]\n",
    "print(video_idxs)\n",
    "\n",
    "input_dir = '../sample_videos/ffpp/real/'\n",
    "print(fpv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super(Ensemble, self).__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "        self.weightage = [1, 1, 1, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = {}\n",
    "        preds = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            pred = model(x.to(device)).cpu().numpy().flatten()\n",
    "            score = expit(pred.mean())\n",
    "            scores[model.__class__.__name__] = score\n",
    "            preds[model.__class__.__name__] = expit(pred)\n",
    "        return scores,preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get path of all models to ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../models/celeb_vit.pth', '../models/celeb_v2.pth', '../models/celeb_v2st.pth', '../models/dfdc_v2.pth', '../models/dfdc_v2st.pth', '../models/ffpp_v2.pth', '../models/ffpp_v2st.pth', '../models/ffpp_vit.pth', '../models/ffpp_vitst.pth']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/ffpp_v2.pth',\n",
       " '../models/ffpp_v2st.pth',\n",
       " '../models/ffpp_vit.pth',\n",
       " '../models/ffpp_vitst.pth']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_paths = glob('../models/**/*.pth', recursive=True)\n",
    "print(model_paths)\n",
    "models_for_dataset = []\n",
    "for i in model_paths:\n",
    "    if(i.split(\"/\")[-1].startswith(dataset)):\n",
    "        models_for_dataset.append(i)\n",
    "models_for_dataset\n",
    "model_paths = models_for_dataset\n",
    "model_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load weights for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26e80f5e4b14392997f6f75c08d00bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Models:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_list = []\n",
    "lim = 0\n",
    "for i in tqdm(model_paths, desc=\"Loading Models\"):\n",
    "    net_name = choices[i.split(\"/\")[2].split(\"_\")[1].split(\".\")[0]]\n",
    "    net_class = getattr(fornet, net_name)\n",
    "    model: FeatureExtractor = net_class().eval().to(device)\n",
    "    model_path = '../models/' + dataset + \\\n",
    "        '_' + i.split('/')[2].split('_')[1]\n",
    "    model.load_state_dict(torch.load(\n",
    "        model_path, map_location='cpu')['net'])\n",
    "    model_list.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf = utils.get_transformer(\n",
    "    face_policy, face_size, model_list[0].get_normalizer(), train=False)\n",
    "facedet = BlazeFace().to(device)\n",
    "facedet.load_weights(\"blazeface/blazeface.pth\")\n",
    "facedet.load_anchors(\"blazeface/anchors.npy\")\n",
    "videoreader = VideoReader(verbose=False)\n",
    "\n",
    "\n",
    "def video_read_fn(x): return videoreader.read_frames(\n",
    "    x, num_frames=frames_per_video)\n",
    "\n",
    "\n",
    "face_extractor = FaceExtractor(video_read_fn=video_read_fn, facedet=facedet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1fb2b88b014f988005ef2817a224a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting faces:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "faces = face_extractor.process_videos(\n",
    "    input_dir=input_dir, filenames=file_names, video_idxs=video_idxs)\n",
    "total_videos = len(video_idxs)\n",
    "\n",
    "\n",
    "faces_frames = [frames_per_video *\n",
    "                x for x in range(0, total_videos+1)]   # [0,32,64,96]\n",
    "\n",
    "faces_hc = torch.stack([transf(image=frame['faces'][0])['image']\n",
    "                       for frame in faces if len(frame['faces'])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_models = Ensemble(model_list).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1e6e55e2d94eceb5b86fbe1102ef29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting::   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = {}\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, total_videos),desc=\"Predicting:\"):  # (0,3) i.e 0,1,2\n",
    "        score,preds = ensemble_models(faces_hc[faces_frames[i]:faces_frames[i+1]])\n",
    "        predictions[input_dir+file_names[video_idxs[i]]] = [score,preds, {'ensemble_score': sum(score.values())/(len(model_list))}, {\n",
    "            'predicted_class': 'real' if sum(score.values())/(len(model_list)) < 0.3 else 'fake', 'true_class': input_dir.split(\"/\")[3]}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'../sample_videos/ffpp/real/043.mp4': [{'TimmV2': 0.012232197,\n",
      "                                         'TimmV2ST': 0.61918104,\n",
      "                                         'ViT': 0.14586504,\n",
      "                                         'ViTST': 0.16916572},\n",
      "                                        {'TimmV2': array([0.00872216, 0.01763926, 0.02246069, 0.01132695, 0.03835763,\n",
      "       0.0131454 , 0.11518755, 0.00667132, 0.01133874, 0.01895693,\n",
      "       0.00206885, 0.00480219, 0.00062077, 0.00691775, 0.00430566,\n",
      "       0.00380525, 0.00092641, 0.01116817, 0.07216255, 0.09467141,\n",
      "       0.00591475, 0.04562845, 0.01535223, 0.03690802, 0.0389526 ,\n",
      "       0.01133066, 0.01979582, 0.06682755, 0.01950602, 0.05097976,\n",
      "       0.00160344, 0.0018279 ], dtype=float32),\n",
      "                                         'TimmV2ST': array([0.4990548 , 0.38916123, 0.7457302 , 0.67451847, 0.5301581 ,\n",
      "       0.81691307, 0.6977466 , 0.40923777, 0.51021624, 0.8374992 ,\n",
      "       0.30644032, 0.795654  , 0.27799407, 0.65075934, 0.71774673,\n",
      "       0.7161587 , 0.25499865, 0.4114892 , 0.8234943 , 0.82831436,\n",
      "       0.35236698, 0.89684206, 0.19344026, 0.17081243, 0.6980605 ,\n",
      "       0.8273754 , 0.8212399 , 0.88731605, 0.9337796 , 0.7259483 ,\n",
      "       0.19495586, 0.48561406], dtype=float32),\n",
      "                                         'ViT': array([0.02773487, 0.01401115, 0.63094515, 0.599878  , 0.3246132 ,\n",
      "       0.07829965, 0.36391762, 0.10086134, 0.1875682 , 0.11991836,\n",
      "       0.23514661, 0.0653054 , 0.03179489, 0.34832165, 0.03682511,\n",
      "       0.15906806, 0.8723015 , 0.29163304, 0.2722573 , 0.05506617,\n",
      "       0.05764351, 0.01866971, 0.02472242, 0.02582176, 0.1274362 ,\n",
      "       0.05781652, 0.0473922 , 0.47048077, 0.39097232, 0.37206325,\n",
      "       0.1913561 , 0.40690023], dtype=float32),\n",
      "                                         'ViTST': array([0.03165095, 0.03295471, 0.19319876, 0.77683973, 0.4556382 ,\n",
      "       0.12018115, 0.07471852, 0.05339048, 0.11872997, 0.09832646,\n",
      "       0.0442314 , 0.11182516, 0.16277592, 0.6377353 , 0.17871428,\n",
      "       0.20853211, 0.2809715 , 0.5047947 , 0.4185147 , 0.07422614,\n",
      "       0.12018778, 0.06211685, 0.08200768, 0.08919311, 0.07351991,\n",
      "       0.13011011, 0.0546464 , 0.24548076, 0.70753765, 0.36162606,\n",
      "       0.16933711, 0.45238596], dtype=float32)},\n",
      "                                        {'ensemble_score': 0.23661099700257182},\n",
      "                                        {'predicted_class': 'real',\n",
      "                                         'true_class': 'real'}],\n",
      " '../sample_videos/ffpp/real/091.mp4': [{'TimmV2': 0.0060049077,\n",
      "                                         'TimmV2ST': 0.07242383,\n",
      "                                         'ViT': 0.007400226,\n",
      "                                         'ViTST': 0.020096418},\n",
      "                                        {'TimmV2': array([0.00669596, 0.01600008, 0.00614436, 0.00555989, 0.00994958,\n",
      "       0.00819115, 0.00424174, 0.00562294, 0.0065126 , 0.00646228,\n",
      "       0.01485843, 0.00316149, 0.00205463, 0.00614114, 0.00776666,\n",
      "       0.00871443, 0.02417853, 0.00317632, 0.00856831, 0.00305019,\n",
      "       0.00388462, 0.00704461, 0.00102011, 0.00079328, 0.00282903,\n",
      "       0.00342236, 0.00426588, 0.01319993, 0.02946681, 0.00344827,\n",
      "       0.02554613, 0.01007028], dtype=float32),\n",
      "                                         'TimmV2ST': array([0.1604439 , 0.05245147, 0.05436828, 0.0416237 , 0.19731379,\n",
      "       0.08235116, 0.03951321, 0.05721328, 0.02997302, 0.07891611,\n",
      "       0.03417044, 0.02930368, 0.04070121, 0.05899001, 0.08478869,\n",
      "       0.04096748, 0.04354064, 0.03807941, 0.04040617, 0.10230298,\n",
      "       0.05952287, 0.08041746, 0.10563913, 0.11035872, 0.10059146,\n",
      "       0.08562938, 0.05604148, 0.11538875, 0.24071893, 0.12839906,\n",
      "       0.30304793, 0.0678308 ], dtype=float32),\n",
      "                                         'ViT': array([0.00187013, 0.00218986, 0.00158507, 0.00614768, 0.00542139,\n",
      "       0.01135541, 0.0022816 , 0.00303027, 0.00386651, 0.00358516,\n",
      "       0.00825989, 0.0028806 , 0.00329399, 0.01816919, 0.02963386,\n",
      "       0.00241493, 0.00152568, 0.3052573 , 0.01288514, 0.00361156,\n",
      "       0.00114313, 0.00278076, 0.00250385, 0.00250683, 0.0015248 ,\n",
      "       0.00654558, 0.00282274, 0.25625038, 0.08090034, 0.00671795,\n",
      "       0.20919847, 0.23927999], dtype=float32),\n",
      "                                         'ViTST': array([0.0101632 , 0.00849138, 0.00656375, 0.01011525, 0.0149502 ,\n",
      "       0.03340577, 0.01987167, 0.03160178, 0.01769096, 0.01629255,\n",
      "       0.00942025, 0.01759659, 0.0196574 , 0.02749974, 0.02250735,\n",
      "       0.01148024, 0.01037023, 0.02961371, 0.03924985, 0.02057485,\n",
      "       0.00565809, 0.01023339, 0.02108554, 0.00731999, 0.00629198,\n",
      "       0.01334628, 0.00740245, 0.09560911, 0.42976308, 0.03022528,\n",
      "       0.23875393, 0.04522868], dtype=float32)},\n",
      "                                        {'ensemble_score': 0.02648134541232139},\n",
      "                                        {'predicted_class': 'real',\n",
      "                                         'true_class': 'real'}]}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['real', 'real']\n",
      "['real', 'real']\n"
     ]
    }
   ],
   "source": [
    "pclass = []\n",
    "tclass = []\n",
    "res = []  # [   [predicted_class,true_class],    [predicted_class,true_class]     ....  ]\n",
    "for preds in predictions:\n",
    "    predicted_class = predictions[preds][-1]['predicted_class']\n",
    "    true_class = predictions[preds][-1]['true_class']\n",
    "    res.append([predicted_class, true_class])\n",
    "    pclass.append(predicted_class)\n",
    "    tclass.append(true_class)\n",
    "print(pclass)\n",
    "print(tclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(pclass)):\n",
    "    if(pclass[i] == 'real'):\n",
    "        pclass[i] = 0\n",
    "    elif(pclass[i] == 'fake'):\n",
    "        pclass[i] = 1\n",
    "\n",
    "pclass = torch.Tensor(pclass)\n",
    "\n",
    "for i in range(0, len(tclass)):\n",
    "    if(tclass[i] == 'real'):\n",
    "        tclass[i] = 0\n",
    "    elif(tclass[i] == 'fake'):\n",
    "        tclass[i] = 1\n",
    "tclass = torch.Tensor(tclass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = torch.stack((tclass, pclass), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmt = torch.zeros(2, 2, dtype=torch.int64)\n",
    "for p in stacked:\n",
    "    tl, pl = p.tolist()\n",
    "    cmt[int(tl), int(pl)] = cmt[int(tl), int(pl)] + 1\n",
    "cmt = cmt.detach().cpu().numpy()\n",
    "cmt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[2 0]\n",
      " [0 0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "names = ('real', 'fake')\n",
    "plt.figure(figsize=(4, 4))\n",
    "plot_confusion_matrix(cmt, names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'video_path': [x for x in predictions.keys()],\n",
    "       'prediction':  pclass   }   \n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save video by annotating frames with frame-level score predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths = glob('../sample_videos/ffpp/real/**/*.mp4', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_idxs = [2,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf = utils.get_transformer(\n",
    "    face_policy, face_size, model_list[0].get_normalizer(), train=False)\n",
    "facedet = BlazeFace().to(device)\n",
    "facedet.load_weights(\"blazeface/blazeface.pth\")\n",
    "facedet.load_anchors(\"blazeface/anchors.npy\")\n",
    "videoreader = VideoReader(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_models = Ensemble(model_list).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5fd74866f7d475f8844d3474d63a45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting faces:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7f84f7890b40478602c0904604a14f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting faces:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract faces:\n",
    "predictions = {}\n",
    "with torch.no_grad():\n",
    "    for a,i in enumerate(fpv):\n",
    "        if(a in video_idxs):\n",
    "            def video_read_fn(x): return videoreader.read_frames(\n",
    "                x, num_frames=i)\n",
    "            face_extractor = FaceExtractor(video_read_fn=video_read_fn, facedet=facedet)\n",
    "            faces = face_extractor.process_video(video_paths[a])\n",
    "            faces_hc = torch.stack([transf(image=frame['faces'][0])['image']\n",
    "                               for frame in faces if len(frame['faces'])])\n",
    "            score,preds = ensemble_models(faces_hc[0:i])\n",
    "            predictions[video_paths[a]] = [score,preds,faces, {'ensemble_score': sum(score.values())/(len(model_list))}, {\n",
    "                    'predicted_class': 'real' if sum(score.values())/(len(model_list)) < 0.3 else 'fake', 'true_class': input_dir.split(\"/\")[3]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_path = 'output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberWithoutRounding(num, precision=4):\n",
    "    [beforeDecimal, afterDecimal] = str(num).split('.')\n",
    "    return beforeDecimal + '.' + afterDecimal[0:precision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,i in enumerate(fpv):\n",
    "    if(a in video_idxs):\n",
    "        frame_count = 0\n",
    "        face_count = 0\n",
    "        writer = None\n",
    "        success = True\n",
    "        vid = cv2.VideoCapture(video_paths[a])\n",
    "        while success:\n",
    "            success, img = vid.read()\n",
    "\n",
    "\n",
    "            if writer is None:\n",
    "                fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "                writer = cv2.VideoWriter(op_path + os.path.basename(video_paths[a]).split(\".\")[0]+'.avi', fourcc, 20, (img.shape[1], img.shape[0]), True)\n",
    "\n",
    "            faces = predictions[video_paths[a]][2]\n",
    "            if(face_count==i):\n",
    "                break\n",
    "            if face_count < i and faces[face_count]['frame_idx'] == frame_count:\n",
    "        \n",
    "                if len(faces[face_count]['detections']) > 0:\n",
    "                    dect = faces[face_count]['detections'][0]\n",
    "\n",
    "                ymin, xmin, ymax, xmax = dect[0], dect[1], dect[2], dect[3]\n",
    "                num_models = len(model_list)\n",
    "                p = {} # contains frame level predictions for each model\n",
    "                for j in preds:\n",
    "                    p[j] = preds[j][face_count]\n",
    "                    \n",
    "                ensemble_pred_score = sum(p.values())/len(p)\n",
    "                \n",
    "\n",
    "                if ensemble_pred_score >= 0.3:\n",
    "                    text = 'Fake:' + numberWithoutRounding(ensemble_pred_score, precision=4)\n",
    "                    rgb = (0, 0, 255)\n",
    "\n",
    "                else:\n",
    "                    text = 'Real:' + numberWithoutRounding(ensemble_pred_score, precision=4)\n",
    "                    rgb = (0, 255, 0)\n",
    "\n",
    "                face_count += 1\n",
    "\n",
    "            cv2.rectangle(img, (xmin, ymin), (xmax, ymax), rgb, 2)\n",
    "            text_y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
    "            cv2.putText(img, text, (int(xmin), int(text_y)), cv2.FONT_HERSHEY_SIMPLEX, 0.75, rgb, 2)\n",
    "            \n",
    "            #cv2.putText(img, t, (10,100), cv2.FONT_HERSHEY_SIMPLEX, 0.75, rgb, 2)\n",
    "            lbls = ['Model']\n",
    "            for n in p:\n",
    "                lbls.append(n)\n",
    "            h,w,c = img.shape\n",
    "            offset = 0\n",
    "            for vv,ll in enumerate(lbls):\n",
    "                if(ll == 'TimmV2'):\n",
    "                    lbls[vv] = 'V2'\n",
    "                elif(ll=='TimmV2ST'):\n",
    "                    lbls[vv] = 'V2ST'\n",
    "            for itr, word in enumerate(lbls):\n",
    "                offset += int(h / len(lbls)) - 10\n",
    "                cv2.putText(img, word, (20, offset), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "                \n",
    "            \n",
    "            lbls_frame_pred = []\n",
    "            for k in p:\n",
    "                lbls_frame_pred.append(p[k])\n",
    "            \n",
    "            lbls_frame_pred_round = ['Score']\n",
    "            for kk in lbls_frame_pred:\n",
    "                lbls_frame_pred_round.append(numberWithoutRounding(kk,4))\n",
    "    \n",
    "            offset = 0\n",
    "            for f, g in enumerate(lbls_frame_pred_round):\n",
    "                if(f==0):\n",
    "                    offset += int(h / len(lbls_frame_pred_round)) - 10\n",
    "                    cv2.putText(img, g, (130, offset), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "                    continue\n",
    "                offset += int(h / len(lbls_frame_pred_round)) - 10\n",
    "                if(float(g) >= 0.3):\n",
    "                    cv2.putText(img, g, (130, offset), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "                elif(float(g) < 0.3):\n",
    "                    cv2.putText(img, g, (130, offset), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "                \n",
    "            writer.write(img)\n",
    "            frame_count += 1\n",
    "\n",
    "        writer.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
